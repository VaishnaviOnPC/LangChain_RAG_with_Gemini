{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWZ3UZsFuxR9MvBH3+24aB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VaishnaviOnPC/LangChain_RAG_with_Gemini/blob/main/RAG_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QA_uKgwtbLHI"
      },
      "outputs": [],
      "source": [
        "!pip install -Uq langchain_community langchain_core langchain-google-genai langchainhub chromadb langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = 'YOUR_API_KEY'"
      ],
      "metadata": {
        "id": "6HrzNqbnD2PB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GEMINI_API_KEY'] = 'YOUR_API_KEY'"
      ],
      "metadata": {
        "id": "OxDPX7zFbaX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.documents import Document\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "XydUTQ0JRjx7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad5d5674-3f18-45c6-e361-571894682df2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_ = (\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\"\"\"\n",
        "for i, doc in enumerate(docs):\n",
        "    print(f\"Document {i+1}:\\n{doc.page_content}\\n{'='*80}\")\n",
        "\"\"\"\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "print(f\"Number of splits: {len(splits)}\")\n",
        "\"\"\"\n",
        "for i, doc in enumerate(splits):\n",
        "    print(f\"Chunk {i+1}:\\n{doc.page_content}\\n{'='*80}\")\n",
        "\"\"\"\n",
        "embedding_model = GoogleGenerativeAIEmbeddings(google_api_key=os.environ.get(\"GEMINI_API_KEY\"), model=\"models/embedding-001\")\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(google_api_key=os.environ.get(\"GEMINI_API_KEY\"), model=\"gemini-2.0-flash\", temperature=0)\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "def format_docs(docs):\n",
        "  return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | output_parser\n",
        ")\n",
        "\n",
        "result = rag_chain.invoke(\"What is Fine-tuning?\")\n",
        "print(f\"Response: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je7ekf71fFHn",
        "outputId": "3cf5d2f8-b386-42b5-d8e9-b99c1a09476c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of splits: 62\n",
            "Response: Fine-tuning is a technique used to improve the capabilities of a pre-trained language model, such as instruction following. It involves training the model on a specific dataset to enhance its performance on a particular task. This process can also introduce new knowledge, though it may increase the model's tendency to hallucinate.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what are my favorite places on Earth?\"\n",
        "document = \"Anywhere with mountains and greenery is one of my favorite places on Earth.\""
      ],
      "metadata": {
        "id": "UsuSiX9yIPze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def estimate_gemini_tokens(text: str) -> int:\n",
        "    \"\"\"\n",
        "    Estimates the number of tokens in a given text for Gemini models.\n",
        "\n",
        "    This is a heuristic approach and will not be perfectly accurate.  It's\n",
        "    intended for rough estimation only.  Relies on word counting and adds\n",
        "    a fudge factor for sub-word tokens and punctuation.\n",
        "\n",
        "    Args:\n",
        "        text: The text string to estimate tokens for.\n",
        "\n",
        "    Returns:\n",
        "        An integer representing the estimated number of tokens.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return 0\n",
        "\n",
        "    # 1. Word Count:  Start with a simple word count.  Use a robust word splitting.\n",
        "    words = re.findall(r'\\b\\w+\\b', text)  # Improved word splitting\n",
        "    word_count = len(words)\n",
        "\n",
        "    # 2. Fudge Factor:  Adjust for sub-word tokens and punctuation.\n",
        "    #    This is the most uncertain part, and needs tuning.\n",
        "    fudge_factor = 0.3  # Adjust this as needed.  Higher = more tokens estimated.\n",
        "    estimated_tokens = int(word_count * (1 + fudge_factor))\n",
        "\n",
        "    # 3. Handle extra whitespace and newlines (optional, may improve accuracy slightly)\n",
        "    extra_chars = len(text) - len(\"\".join(words))\n",
        "    estimated_tokens += int(extra_chars / 4) # Assume roughly 4 chars per token\n",
        "\n",
        "    return estimated_tokens\n",
        "\n",
        "def test_estimation(text: str):\n",
        "    \"\"\"\n",
        "    Tests the token estimation function and prints the result\n",
        "    \"\"\"\n",
        "    estimated_count = estimate_gemini_tokens(text)\n",
        "    print(f\"Text: \\\"{text}\\\"\")\n",
        "    print(f\"Estimated tokens: {estimated_count}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run test cases\n",
        "    \"\"\"\n",
        "    # Some test cases\n",
        "    test_estimation(question)\n",
        "    test_estimation(document)\n",
        "    \"\"\"\n",
        "    test_estimation(\"This is a simple sentence.\")\n",
        "    test_estimation(\"Unbelievably complex, and very long-winded, isn't it?\")\n",
        "    test_estimation(\"你好世界\")  # Test with non-ASCII characters\n",
        "    test_estimation(\"1234567890\") # Test with numbers\n",
        "    test_estimation(\"   Extra   spaces   here   \") # Test with extra spaces\n",
        "    test_estimation(\"Line 1\\nLine 2\\nLine 3\") # Test with newlines\n",
        "    test_estimation(\"\") # Test with empty string\n",
        "    test_estimation(\"a b c d e f g h i j k l m n o p q r s t u v w x y z\") # Test with many single letters\n",
        "    \"\"\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryoi6driY27o",
        "outputId": "14039cce-d9bc-418e-d9da-a1eb68633aa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: \"what are my favorite places on Earth?\"\n",
            "Estimated tokens: 10\n",
            "Text: \"Anywhere with mountains and greenery is one of my favorite places on Earth.\"\n",
            "Estimated tokens: 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = GoogleGenerativeAIEmbeddings(google_api_key=os.environ.get(\"GEMINI_API_KEY\"), model=\"models/embedding-001\")\n",
        "query_result = embedding.embed_query(question)\n",
        "document_result = embedding.embed_query(document)\n",
        "len(query_result), len(document_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UPOK8glSoTM",
        "outputId": "eb65e90e-6b0a-442f-c697-988418a7699c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "similarity = cosine_similarity(query_result, document_result)\n",
        "print(\"Cosine Similarity:\", similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEciFI4nVFW1",
        "outputId": "e4c3a359-89da-4f3d-ce0d-9bbd0feabf7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 0.7928199970798053\n"
          ]
        }
      ]
    }
  ]
}